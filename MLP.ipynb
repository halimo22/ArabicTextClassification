{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>Negative</td>\n",
       "      <td>معرفش ليه كنت عاوزة أكملها وهي مش عاجباني من ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>Negative</td>\n",
       "      <td>لا يستحق ان يكون في بوكنق لانه سيئ . لا شي. لا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>Negative</td>\n",
       "      <td>كتاب ضعيف جدا ولم استمتع به. فى كل قصه سرد لحا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>Negative</td>\n",
       "      <td>مملة جدا. محمد حسن علوان فنان بالكلمات، والوصف...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>Negative</td>\n",
       "      <td>لن ارجع إليه مرة اخرى . قربه من البحر. المكان ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                               text\n",
       "0      Positive  ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...\n",
       "1      Positive  أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...\n",
       "2      Positive  هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...\n",
       "3      Positive  خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...\n",
       "4      Positive  ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...\n",
       "...         ...                                                ...\n",
       "99994  Negative  معرفش ليه كنت عاوزة أكملها وهي مش عاجباني من ا...\n",
       "99995  Negative  لا يستحق ان يكون في بوكنق لانه سيئ . لا شي. لا...\n",
       "99996  Negative  كتاب ضعيف جدا ولم استمتع به. فى كل قصه سرد لحا...\n",
       "99997  Negative  مملة جدا. محمد حسن علوان فنان بالكلمات، والوصف...\n",
       "99998  Negative  لن ارجع إليه مرة اخرى . قربه من البحر. المكان ...\n",
       "\n",
       "[99999 rows x 2 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(f'Dataset.tsv', sep='\\t')  \n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arabic Words Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "احب رمج علم عصر حدث\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/seif/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Arabic stopwords\n",
    "nltk.download('stopwords')\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "def preprocess_arabic_text(text):\n",
    "    text = re.sub(r'[\\u064B-\\u0652]', '', text)  \n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    \n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in arabic_stopwords]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test\n",
    "sample_text = \"أحب البرمجة والتعلم في هذا العصر الحديث.\"\n",
    "cleaned_text = preprocess_arabic_text(sample_text)\n",
    "print(cleaned_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Aravec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = gensim.models.Word2Vec.load('models/full_grams_cbow_300_twitter.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_embedd_arabic_text(text):\n",
    "    text = re.sub(r'[\\u064B-\\u0652]', '', text)  \n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text) \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "\n",
    "    tokens = text.split()\n",
    "    filtered_sentence=[]\n",
    "    for word in tokens:\n",
    "        if word not in arabic_stopwords:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            filtered_sentence.append(stemmed_word)\n",
    "    if len(filtered_sentence) == 0:\n",
    "        return np.zeros(t_model.vector_size)\n",
    "    return t_model.wv.get_mean_vector(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_vector'] = df['text'].apply(preprocess_embedd_arabic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...</td>\n",
       "      <td>[0.03075951, -0.023665147, -0.0020882313, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...</td>\n",
       "      <td>[0.032468032, -0.0050334274, 0.013271037, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...</td>\n",
       "      <td>[0.03640272, -0.0052244263, 0.0027546007, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...</td>\n",
       "      <td>[0.039948247, -0.0121723525, 0.00025296805, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...</td>\n",
       "      <td>[0.0050339643, -0.0018979385, 0.0238677, 0.033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>Negative</td>\n",
       "      <td>معرفش ليه كنت عاوزة أكملها وهي مش عاجباني من ا...</td>\n",
       "      <td>[0.021896569, -0.028480032, 0.016720144, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>Negative</td>\n",
       "      <td>لا يستحق ان يكون في بوكنق لانه سيئ . لا شي. لا...</td>\n",
       "      <td>[0.057995524, -0.018431652, 0.01127016, -0.000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>Negative</td>\n",
       "      <td>كتاب ضعيف جدا ولم استمتع به. فى كل قصه سرد لحا...</td>\n",
       "      <td>[0.020529604, 0.014629706, -0.011728142, 0.011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>Negative</td>\n",
       "      <td>مملة جدا. محمد حسن علوان فنان بالكلمات، والوصف...</td>\n",
       "      <td>[0.03499831, 0.0057043405, 0.007058305, 0.0093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>Negative</td>\n",
       "      <td>لن ارجع إليه مرة اخرى . قربه من البحر. المكان ...</td>\n",
       "      <td>[0.034981553, -0.03346727, 0.02089013, 0.00287...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99836 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                               text  \\\n",
       "0      Positive  ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...   \n",
       "1      Positive  أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...   \n",
       "2      Positive  هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...   \n",
       "3      Positive  خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...   \n",
       "4      Positive  ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...   \n",
       "...         ...                                                ...   \n",
       "99994  Negative  معرفش ليه كنت عاوزة أكملها وهي مش عاجباني من ا...   \n",
       "99995  Negative  لا يستحق ان يكون في بوكنق لانه سيئ . لا شي. لا...   \n",
       "99996  Negative  كتاب ضعيف جدا ولم استمتع به. فى كل قصه سرد لحا...   \n",
       "99997  Negative  مملة جدا. محمد حسن علوان فنان بالكلمات، والوصف...   \n",
       "99998  Negative  لن ارجع إليه مرة اخرى . قربه من البحر. المكان ...   \n",
       "\n",
       "                                             text_vector  \n",
       "0      [0.03075951, -0.023665147, -0.0020882313, 0.00...  \n",
       "1      [0.032468032, -0.0050334274, 0.013271037, 0.00...  \n",
       "2      [0.03640272, -0.0052244263, 0.0027546007, -0.0...  \n",
       "3      [0.039948247, -0.0121723525, 0.00025296805, 0....  \n",
       "4      [0.0050339643, -0.0018979385, 0.0238677, 0.033...  \n",
       "...                                                  ...  \n",
       "99994  [0.021896569, -0.028480032, 0.016720144, -0.00...  \n",
       "99995  [0.057995524, -0.018431652, 0.01127016, -0.000...  \n",
       "99996  [0.020529604, 0.014629706, -0.011728142, 0.011...  \n",
       "99997  [0.03499831, 0.0057043405, 0.007058305, 0.0093...  \n",
       "99998  [0.034981553, -0.03346727, 0.02089013, 0.00287...  \n",
       "\n",
       "[99836 rows x 3 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['text_vector'].apply(lambda x: not np.array_equal(x, np.zeros(t_model.vector_size)))]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10906/1413173990.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_id'] = df['label'].map({'Negative': 0, 'Positive': 1, 'Mixed': 2})\n"
     ]
    }
   ],
   "source": [
    "df['sentiment_id'] = df['label'].map({'Negative': 0, 'Positive': 1, 'Mixed': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "99994    0\n",
       "99995    0\n",
       "99996    0\n",
       "99997    0\n",
       "99998    0\n",
       "Name: sentiment_id, Length: 99836, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment_id'], test_size=0.1)\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(df['label'])\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(t_model.vector_size,)))  # Input layer\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation='relu'))  \n",
    "model.add(Dropout(0.2))  # Dropout layer\n",
    "model.add(Dense(128, activation='relu'))  # Hidden layer\n",
    "model.add(Dropout(0.5))  # Dropout layer\n",
    "model.add(Dense(64, activation='relu'))  # Hidden layer\n",
    "model.add(Dense(len(np.unique(labels_encoded)), activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">77,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m77,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_54 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_55 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">119,603</span> (467.20 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m119,603\u001b[0m (467.20 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">119,003</span> (464.86 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m119,003\u001b[0m (464.86 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> (2.34 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m600\u001b[0m (2.34 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack(X_train)\n",
    "X_test = np.stack(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5084 - loss: 0.9682\n",
      "Epoch 1: val_accuracy improved from -inf to 0.59404, saving model to best_model.keras\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5084 - loss: 0.9682 - val_accuracy: 0.5940 - val_loss: 0.8539\n",
      "Epoch 2/15\n",
      "\u001b[1m1341/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5826 - loss: 0.8684\n",
      "Epoch 2: val_accuracy improved from 0.59404 to 0.60160, saving model to best_model.keras\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5826 - loss: 0.8684 - val_accuracy: 0.6016 - val_loss: 0.8428\n",
      "Epoch 3/15\n",
      "\u001b[1m1325/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5927 - loss: 0.8476\n",
      "Epoch 3: val_accuracy improved from 0.60160 to 0.61128, saving model to best_model.keras\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5927 - loss: 0.8475 - val_accuracy: 0.6113 - val_loss: 0.8273\n",
      "Epoch 4/15\n",
      "\u001b[1m1322/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6026 - loss: 0.8309\n",
      "Epoch 4: val_accuracy improved from 0.61128 to 0.61318, saving model to best_model.keras\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6026 - loss: 0.8310 - val_accuracy: 0.6132 - val_loss: 0.8251\n",
      "Epoch 5/15\n",
      "\u001b[1m1324/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6095 - loss: 0.8163\n",
      "Epoch 5: val_accuracy improved from 0.61318 to 0.61574, saving model to best_model.keras\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6096 - loss: 0.8163 - val_accuracy: 0.6157 - val_loss: 0.8189\n",
      "Epoch 6/15\n",
      "\u001b[1m1326/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6208 - loss: 0.8032\n",
      "Epoch 6: val_accuracy did not improve from 0.61574\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6208 - loss: 0.8032 - val_accuracy: 0.6154 - val_loss: 0.8224\n",
      "Epoch 7/15\n",
      "\u001b[1m1328/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6277 - loss: 0.7882\n",
      "Epoch 7: val_accuracy did not improve from 0.61574\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6276 - loss: 0.7883 - val_accuracy: 0.6115 - val_loss: 0.8147\n",
      "Epoch 8/15\n",
      "\u001b[1m1338/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6309 - loss: 0.7794\n",
      "Epoch 8: val_accuracy did not improve from 0.61574\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6309 - loss: 0.7794 - val_accuracy: 0.6116 - val_loss: 0.8268\n",
      "Epoch 9/15\n",
      "\u001b[1m1328/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6389 - loss: 0.7686\n",
      "Epoch 9: val_accuracy did not improve from 0.61574\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6389 - loss: 0.7687 - val_accuracy: 0.6116 - val_loss: 0.8166\n",
      "Epoch 10/15\n",
      "\u001b[1m1331/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6479 - loss: 0.7565\n",
      "Epoch 10: val_accuracy did not improve from 0.61574\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6478 - loss: 0.7567 - val_accuracy: 0.6093 - val_loss: 0.8193\n",
      "Epoch 11/15\n",
      "\u001b[1m1337/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6496 - loss: 0.7486\n",
      "Epoch 11: val_accuracy improved from 0.61574 to 0.61651, saving model to best_model.keras\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6496 - loss: 0.7487 - val_accuracy: 0.6165 - val_loss: 0.8217\n",
      "Epoch 12/15\n",
      "\u001b[1m1338/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6569 - loss: 0.7409\n",
      "Epoch 12: val_accuracy did not improve from 0.61651\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6569 - loss: 0.7409 - val_accuracy: 0.6116 - val_loss: 0.8214\n",
      "Epoch 13/15\n",
      "\u001b[1m1318/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6626 - loss: 0.7304\n",
      "Epoch 13: val_accuracy did not improve from 0.61651\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6625 - loss: 0.7306 - val_accuracy: 0.6110 - val_loss: 0.8309\n",
      "Epoch 14/15\n",
      "\u001b[1m1341/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6613 - loss: 0.7287\n",
      "Epoch 14: val_accuracy did not improve from 0.61651\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6613 - loss: 0.7287 - val_accuracy: 0.6111 - val_loss: 0.8281\n",
      "Epoch 15/15\n",
      "\u001b[1m1330/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6699 - loss: 0.7099\n",
      "Epoch 15: val_accuracy did not improve from 0.61651\n",
      "\u001b[1m1348/1348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6699 - loss: 0.7100 - val_accuracy: 0.6093 - val_loss: 0.8294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ba296b1bb90>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=60, validation_split=0.1, callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 817us/step - accuracy: 0.6049 - loss: 0.8310\n",
      "Test Loss: 0.8310883641242981, Test Accuracy: 0.6073718070983887\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Mixed       0.63      0.68      0.66      3356\n",
      "    Negative       0.64      0.67      0.65      3290\n",
      "    Positive       0.54      0.47      0.50      3338\n",
      "\n",
      "    accuracy                           0.61      9984\n",
      "   macro avg       0.60      0.61      0.60      9984\n",
      "weighted avg       0.60      0.61      0.60      9984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model('best_model.keras')\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  \n",
    "\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
